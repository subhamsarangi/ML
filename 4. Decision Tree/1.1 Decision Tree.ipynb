{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "#### Decision Trees are versatile Machine Learning algorithms that can perform both classification and regression tasks. They are very powerful algorithms, capable of fitting complex datasets.\n",
    "We can think of this model as breaking down our data by making decisions based on asking a series of questions. Based on the features in our training set, the decision tree model learns a series of questions to infer the class labels of the samples.\n",
    "\n",
    "Using the decision algorithm, we start at the tree root and split the data on the feature that results in the largest ---*information gain (IG)*. In an iterative process, we can then repeat this splitting procedure at each child node until the leaves are pure. This means that the samples at each node all belong to the same class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximizing information gain\n",
    "In order to split the nodes at the most informative features, we need to defne an objective function that we want to optimize via the tree learning algorithm. Here, our objective function is to maximize the information gain at each split, which we defne as follows:\n",
    "\n",
    "### $IG\\left( \\mathbf{D}_p,f\\right) =  I\\left(\\mathbf{D}_p\\right)- \\sum_{j=1}^m \\frac{\\mathbf{N}_j}{\\mathbf{N}_p}I\\left(\\mathbf{D}_p\\right)$\n",
    "\n",
    "As we can see, the information gain is simply the difference between the _impurity_ of the parent node and the sum of the child node impurities — the lower the impurity of the child nodes, the larger the information gain.\n",
    "\n",
    "Now, the three impurity measures or splitting criteria that are commonly used in binary decision trees are Gini index ($I_G$), entropy ($I_H$), and the classifcation error ($I_E$).\n",
    "\n",
    "## Entropy\n",
    "### $I_H(t) = - \\sum_{i=1}^c p{(i\\:|\\:t)}\\:log_2\\:p{(i\\:|\\:t)}$\n",
    "Here, $p{(i\\:|\\:t)}$ is the proportion of the samples that belongs to class __c__ for a particular node __t__. The entropy is therefore 0 if all samples at a node belong to the same class, and the entropy is maximal if we have a uniform class distribution. \n",
    "\n",
    "For example, in a binary class setting, the entropy is 0 if $p{(i=1|t)}=1$ or $p{(i=0|t)}=0$. If the classes are distributed uniformly with $p{(i=1|t)}=0.5$ and $p{(i=0|t)}=0.5$ , the entropy is 1. Therefore, we can say that the entropy criterion attempts to maximize the mutual information in the tree.\n",
    "\n",
    "## Gini index\n",
    "Gini index can be understood as a criterion to minimize the probability of misclassifcation:\n",
    "### $I_G(t) =1 - \\sum_{i=1}^c p{(i\\:|\\:t)}^2$\n",
    "Similar to entropy, the Gini index is maximal if the classes are perfectly mixed.\n",
    "\n",
    "## Classifcation error\n",
    "### $I_E = 1 - max{\\{p(i\\:|\\:t)}\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('Classified Data', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2,criterion='entropy')\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file=\"iris_dtree.dot\",\n",
    "    feature_names=iris.feature_names[2:],\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the .dot file to a PNG using the dot commandline tool from the graphviz package is done with the following command...\n",
    "\n",
    "**`dot -Tpng iris_tree.dot -o iris_tree.png`**\n",
    "![img](iris_dtree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating features and Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing/Scaling\n",
    "One of the many qualities of Decision Trees is that they require very little data preparation. In particular, they don’t require feature scaling or centering at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset\n",
    "While experimenting with any learning algorithm, it is important not to test the prediction of an estimator on the data used to fit the estimator as this would not be evaluating the performance of the estimator on new data. This is why datasets are often split into train and test data.\n",
    "#### A random permutation, to split the data randomly\n",
    "```python\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(X))\n",
    "X_train = X[indices[:-20]]\n",
    "y_train = y[indices[:-20]]\n",
    "X_test = X[indices[-20:]]\n",
    "y_test = y[indices[-20:]]\n",
    "```\n",
    "#### But we will use the `train_test_split` function from `sklearn.model_selection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#    X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and fit a    Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
