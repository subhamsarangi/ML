{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)\n",
    "***Objective***: To maximize the **Margin** which is the distance between the separating **hyperplane** (i.e. decision boundary) and the **Support Vectors** (i.e. the training samples that are closest to this hyperplane).\n",
    "\n",
    "- ***Q***: Why do we need _decision boundaries with large margins_?\n",
    "- ***A***: They tend to have a _lower generalization error_ (whereas models with small margins are more prone to overfitting.)\n",
    "\n",
    ">The _positive_ and _negative_ hyperplanes that are parallel to the decision boundary, can be expressed as follows:\n",
    "$$w_0 + w^Tx_{pos} = 1$$\n",
    "$$w_0 + w^Tx_{neg} = -1$$\n",
    "\n",
    ">>By subtracting those two linear equations from each other, we get:\n",
    "$$\\Rightarrow w^T(x_{pos}-x_{neg}) =2$$\n",
    "\n",
    ">>We can **normalize** this by the length of the vector $w$, which is defned as follows:\n",
    "$$\\|w\\| =\\sqrt{\\sum_{j=1}^{m} w_j^2}$$\n",
    "\n",
    ">So we arrive at the following equation:\n",
    "### $$\\frac{w^T(x_{pos}-x_{neg})}{\\|w\\|}=\\frac{2}{\\|w\\|}$$\n",
    "The left side of the equation can be interpreted as the _distance between the positive and negative hyperplane_ (i.e the **margin** that we want to maximize).\n",
    "\n",
    "Now,\n",
    "\n",
    "***Objective function of the SVM***:  The maximization of this margin by maximizing $\\color{purple}{\\frac{2}{\\|w\\|}}$ under the constraint that the samples are classifed correctly, which can be written as follows:\n",
    "$$w_0+w^Tx^{(i)}\\geq1 \\;if\\:y^{(i)} = 1$$\n",
    "$$w_0+w^Tx^{(i)}<-1 \\;if\\:y^{(i)} = -1$$\n",
    "\n",
    "This can also be written more compactly as follows:\n",
    "### $$y^{(i)}(w_0+w^Tx^{(i)})\\geq 1\\forall_i$$\n",
    "\n",
    "In practice, though, it is easier to minimize the reciprocal term $\\color{purple}{\\frac{1}{2}\\|w\\|^2}$\n",
    "\n",
    "***Dealing with nonlinearly separable data using slack variables***: Slack variable, $\\xi$ was introduced because linear constraints need to be relaxed for nonlinearly separable data to allow \"_convergence of the optimization in the presence of misclassifcations under the appropriate cost penalization_.\" \n",
    "The positive-values slack variable is simply added to the linear constraints:\n",
    "$$w^Tx^{(i)}\\geq \\;if\\:y^{(i)} = 1 - \\xi^{(i)}$$\n",
    "$$w^Tx^{(i)}< -1 \\;if\\:y^{(i)} = 1 + \\xi^{(i)}$$\n",
    "\n",
    "So\n",
    "\n",
    "**The new objective to be minimized:** \n",
    "### $$\\frac{1}{2}\\|w\\|^2 + C(\\sum_{i}\\xi^{(i)})$$\n",
    "Using the variable $C$, we can then control the penalty for misclassifcation. Large values of $C$ correspond to large error penalties, whereas, we are less strict about misclassifcation errors if we choose smaller values for $C$. We can then we use the parameter $C$ to control the width of the margin and therefore tune the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Classified Data', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating features and Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing/Scaling\n",
    "`sklearn.preprocessing` package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators.\n",
    "\n",
    "*Standardization of datasets* is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: *Gaussian with zero mean and unit variance.*\n",
    "\n",
    "In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset\n",
    "While experimenting with any learning algorithm, it is important not to test the prediction of an estimator on the data used to fit the estimator as this would not be evaluating the performance of the estimator on new data. This is why datasets are often split into train and test data.\n",
    "#### A random permutation, to split the data randomly\n",
    "```python\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(X))\n",
    "X_train = X[indices[:-20]]\n",
    "y_train = y[indices[:-20]]\n",
    "X_test = X[indices[-20:]]\n",
    "y_test = y[indices[-20:]]\n",
    "```\n",
    "#### But we will use the `train_test_split` function from `sklearn.model_selection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and fit a    Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
